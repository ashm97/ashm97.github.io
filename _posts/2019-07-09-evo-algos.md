---
title: Feature Selection with Evolutionary Algorithms - Natural Selection on Steroids!
output:
  md_document:
    variant: gfm
    preserve_yaml: TRUE
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../_posts") })
date: 2019-07-09
permalink: /posts/2019/07/evo-algos/
excerpt_separator: <!--more-->
toc: true
tags:
  - Feature Selection
  - Evolutionary Algorithms
---



Many artificial systems already borrow from nature, take planes modelled after birds, or artificial intelligence itself with their naturally inspired neural networks. Developed in the mid-1960s, Evolutionary Algorithms (EA) are essentially doing the same. Being stochastic search algorithms, they are inspired by the basic principles of biological evolution and natural selection. Algorithmically defining this procedure, allows us to extend it to optimizing feature sets of Machine Learning models in high dimensional data. A space where traditional means of optimizing feature sets are comparatively slow or rigid. Furthermore, doing some artificial guiding, like that of a divine power, we can extend this concept to escape local optimums and better approximate a global optimal solution. In the following article I will provide a brief outline of an EA, drawing from my experience of working with the R Galgo [1] package for genetic data.
.

<!--more-->

<img src="/images/posts/pdf-data/bar_plot-1.png" width="75%" style="display: block; margin: auto;" />


## Natural Selection

Let’s first start off with reminding ourselves what natural selection is. Well known in the literature, natural selection is the recursive process whereby organisms better adapted to their environment tend to survive and produce more offspring. Over several, in some cases many thousands of generations this process converges to a ‘fitter’ i.e more adapted collective population. Applying the general outline in an algorithmic fashion we can capture the procedure by several steps to employ computationally.

## Initial Population

EAs generally work over a population of models, defined by some population size parameter. Defining the populations makeup is the algorithms first step and is often generated randomly. Within this population models each have chromosomes, which consist of a number of features, replacing the genes from the natural phenomena.

<img src="/images/posts/pdf-data/bar_plot-1.png" width="75%" style="display: block; margin: auto;" />

## Fitness Function

With the population of models, the algorithm evaluates all models defined by some fitness function. This is evaluated for each of the models as a performance metric on the given data set. This fitness score then defines how likely the model is to be reproduced and produce offspring. Hence, less fit models will have a lower probability of producing similar offspring. This effect of eliminating of less fit models’ compounds over time as the procedure is repeated and allows the algorithm to converge to fitter populations.

## Selection

Selection is the embodiment of preventing fewer fit models to reproduce, and the opposite for fitter models. This can be done my simply altering probabilities, or by reproducing the population of models proportional to their score.

## Crossover

The crossover is a significant phase in the EA, by testing different pairs of chromones in combination it considers more multivariate relationships present within model features. Chromosomes containing genes with high predictive power will be retained in the overall population, and considered with many of combinations of genes, to see if greater performance can be achieved.

## Mutation

Mutations, changes to model chromosome features, are used to maintain genetic diversity by introducing some parameterized stochasticity. This stochasticity attempts to avoid local optimums by preventing the population from becoming too similar, and hence considering more feature interactions that may explore more areas of the possible solution space. However, it is well noted that setting the mutation probability to high results in simply a primitive random search.

## Termination

The final step is termination, given the population has converged to some fitness threshold, or the generation limit has been reached. If the two previous conditions have not been met, the population continues to repeat the 3 previous steps in a loop.

<img src="/images/posts/pdf-data/bar_plot-1.png" width="75%" style="display: block; margin: auto;" />

## Final Thoughts

From working with EAs to discover biomarkers I’d like to give some final thoughts as to what I find particularly interesting in the world of ESs.

Firstly, there is a big advantage in large search spaces, with complex interactions between features over traditional forward, backwards feature selection strategies. By considering lots of small models, and many different sets of interacting features, we can find some quite good performing small models. And it’s a well-known axiom that similar models are often better approximations.

Secondly, there are some quite cool workarounds for escaping the realm of local optimums. Already, mentioned, the random mutations allow us to jump out of local confinements. Moreover, in implementation it is common to initialise a number of different populations, in parallel searches, to explore more of the potential solution space. This has the similar process of two separated populations of species evolving in nature, where sub-populations are created and initially exist in separate niches of a search space, like a river preventing two populations from intermixing. However, deployment with the computational approach allows us to not just subgroup the same population but allows us to initialize many hundreds of different populations across the search space.

Thirdly, unlike natural evolution we have the opportunity to improve the performance with extra statistical testing. In Galgo, we can run a final round of feature selection of the population model space, by only considering the subset of feature found through the EAs procedure. In my case I found this to be an essential step, as it provided a more flexible model size, as the search procedure was good at finding lots of smaller models which perform well, but would of suffered in terms of performance by inc4asing the chromones size in the search.

Finally, building on the concept of separate sub populations, one thing I’m quite interested now is island the idea of island evolution and how we can encode some leakage parameters, in which a number of individuals are exchanged between separate sub populations, as would happen in the natural world. Which would thus, allow maintenance of greater genetic diversity.

I really do hope to explore EA’s in the future as they’ve been a really exciting research topic for myself. I look forward to hearing how you guys in the future will implement and augment them!

[1]  Trevino, V. and F. Falciani, GALGO: an R package for multivariate variable selection using genetic algorithms.Bioinformatics, 2006. 22(9): p. 1154-6
